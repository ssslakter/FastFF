# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_performance.ipynb.

# %% auto 0
__all__ = ['FLATTEN_TFM', 'get_mnist_ds', 'train_mnist', 'run_sweep', 'FFFLeavesDistCB', 'GetGradCB']

# %% ../nbs/02_performance.ipynb 4
import wandb
from fastai.vision.all import *
from fastai.callback.wandb import *
import torchvision.transforms as T
from torchvision.datasets import MNIST

# %% ../nbs/02_performance.ipynb 6
FLATTEN_TFM = T.Lambda(lambda x: x.view(-1,28*28).squeeze())
def get_mnist_ds(tfms = FLATTEN_TFM):
    mean, std = 0.130652368068695068, 0.307504087686538696
    tfms = tfms if isinstance(tfms,list) else [tfms]
    tfm = T.Compose([T.ToTensor(),T.Normalize((mean,), (std,)), *tfms])
    train = MNIST('../data', train=True, download=True,transform=tfm)
    test = MNIST('../data', train=False, download=True,transform=tfm)
    return train, test

def train_mnist(model, bs=256, epochs=10, lr_max=3e-2, cbs=None, loss = F.cross_entropy, metric = accuracy):
    ds = get_mnist_ds()
    dls = DataLoaders(TfmdDL(ds[0], bs, True),TfmdDL(ds[1], bs, False))
    learn = Learner(dls, model, loss_func=loss, metrics=metric, opt_func = Adam)
    learn.fit_one_cycle(epochs, lr_max=lr_max, cbs=cbs)
    return learn

# %% ../nbs/02_performance.ipynb 10
def run_sweep(train_ds, test_ds, prm_model,sweep_cfg,
              metrics = accuracy,
              loss = F.cross_entropy,
              opt = Adam,
              cfg = None,
              before_learn_cb=None, project=None, count=5, seed=0):
    def _f(cfg=cfg):
        with wandb.init(config=cfg):
            if seed is not None: set_seed(seed)
            cfg = wandb.config
            model = prm_model(**cfg.model)
            dls = DataLoaders(TfmdDL(train_ds, cfg.bs, True),TfmdDL(test_ds, cfg.bs, False))
            learn = Learner(dls, model, loss_func=loss, metrics=metrics, opt_func = opt)
            if before_learn_cb: before_learn_cb(learn, model, dls)
            learn.fit_one_cycle(cfg.epochs, **cfg.learn)
    
    sweep_id = wandb.sweep(sweep_cfg, project=project)
    wandb.agent(sweep_id, _f, count=count)

# %% ../nbs/02_performance.ipynb 16
class FFFLeavesDistCB(Callback):
    '''Gets data from leaves of FFF module and logs to wandb if enabled'''
    def __init__(self, leaves_attr, wandb=False, module=None, sample_size=30):
        store_attr()
 
    def before_fit(self): 
        self.module = ifnone(self.module, self.learn.model)
        if self.wandb: self._wandb_step = wandb.run.step - 1
        self.data = []

    def before_epoch(self): 
        self.tree_leaves, self.preds = [], []
        self.xs, self.ys = [], []
    
    def after_batch(self): 
        if self.training: 
            if self.wandb:  self._wandb_step += 1
            self.tree_leaves.append(getattr(self.module,self.leaves_attr).argmax(1).squeeze())
            self.preds.append(self.pred.argmax(1))
            self.xs.append(self.xb[0]), self.ys.append(self.yb[0])
             
    def after_epoch(self):
        leaves, preds = torch.cat(self.tree_leaves),torch.cat(self.preds)
        xs, ys = torch.cat(self.xs), torch.cat(self.ys)
        self.data.append((xs,ys,leaves,preds))
        if self.wandb: 
            fig, axs = subplots()
            self.show(-1, ax=axs[0], show=False)
            handles, labels = axs[0].get_legend_handles_labels()
            fig.legend(handles, labels)
            wandb.log({"Leaf distribution": fig}, step = self._wandb_step)
    
    def after_fit(self):
        leaves = torch.cat(self.tree_leaves)
        idx = torch.randperm(leaves.size(0))[:self.sample_size]
        xs, ys, preds = torch.cat(self.xs), torch.cat(self.ys), torch.cat(self.preds)
        xs, ys, preds, leaves = xs[idx], ys[idx], preds[idx], leaves[idx]
        if self.wandb:
            cols = ['leaf','image','pred','target']
            data = [[l,wandb.Image(i.view(28,28)),p,y] for l,i,p,y in zip(leaves,xs,preds,ys)]
            wandb.log({"samples": wandb.Table(data=data, columns=cols)})
    
    def show(self, epoch_idx, ax=None, show=True):
        leaves, lbls = self.data[epoch_idx][2], self.data[epoch_idx][1]
        if not ax: ax = subplots()[1][0]
        bottom, bins = torch.zeros(self.module.n_leaves), L(range(self.module.n_leaves)).map(str)
        for d in range(10):
            hist = torch.bincount(leaves[lbls==d], minlength=self.module.n_leaves)
            ax.bar(bins, hist, label=str(d),bottom=bottom)
            bottom += hist
        if show: 
             ax.legend(); plt.show()


# %% ../nbs/02_performance.ipynb 19
class GetGradCB(Callback):
    '''Get grads from module attributes'''
    def __init__(self, modules: list): self.modules = modules
    def before_fit(self): 
        self.grads = {m:[] for m in self.modules}

    def after_fit(self): self.grads = {k:torch.stack(v) for k, v in self.grads.items()}
    
    def after_backward(self):
        for mod in self.modules:
            grad = getattr(mod, 'weight',mod)
            grad = getattr(grad, 'grad', None)
            if grad is None: continue
            self.grads[mod].append(torch.clone(grad.detach()))

    def show(self, mod, normalize, ax=None, figsize=(8,6)):
        if not ax: ax = subplots(figsize=figsize)[1][0]
        grads = normalize(self.grads[mod])
        for i in range(grads.shape[-1]): ax.plot(grads[:,i],label=i)
        ax.legend()
