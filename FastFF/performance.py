# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_performance.ipynb.

# %% auto 0
__all__ = ['run_sweep', 'FFFLeavesDistCB', 'GetGradCB']

# %% ../nbs/02_performance.ipynb 4
import wandb
from fastai.vision.all import *
from fastai.callback.wandb import *

# %% ../nbs/02_performance.ipynb 10
def run_sweep(train_ds, test_ds, prm_model,sweep_cfg,
              metrics = accuracy,
              loss = F.cross_entropy,
              opt = Adam,
              cfg = None,
              before_learn_cb=None, project=None, count=5, seed=0):
    def _f(cfg=cfg):
        with wandb.init(config=cfg):
            if seed is not None: set_seed(seed)
            cfg = wandb.config
            model = prm_model(**cfg.model)
            dls = DataLoaders(TfmdDL(train_ds, cfg.bs, True),TfmdDL(test_ds, cfg.bs, False))
            learn = Learner(dls, model, loss_func=loss, metrics=metrics, opt_func = opt)
            if before_learn_cb: before_learn_cb(learn, model, dls)
            learn.fit_one_cycle(cfg.epochs, **cfg.learn)
    
    sweep_id = wandb.sweep(sweep_cfg, project=project)
    wandb.agent(sweep_id, _f, count=count)

# %% ../nbs/02_performance.ipynb 16
class FFFLeavesDistCB(Callback):
    '''Gets data from leaves of FFF module and logs to wandb if enabled'''
    def __init__(self, leaves_attr, wandb=False, module=None, sample_size=30):
        store_attr()
 
    def before_fit(self): 
        self.module = ifnone(self.module, self.learn.model)
        if self.wandb: self._wandb_step = wandb.run.step - 1
        self.data = []

    def before_epoch(self): 
        self.tree_leaves, self.preds = [], []
        self.xs, self.ys = [], []
    
    def after_batch(self): 
        if self.training: 
            if self.wandb:  self._wandb_step += 1
            self.tree_leaves.append(getattr(self.module,self.leaves_attr).argmax(1))
            self.preds.append(self.pred.argmax(1))
            self.xs.append(self.xb[0]), self.ys.append(self.yb[0])
             
    def after_epoch(self):
        leaves, preds = torch.cat(self.tree_leaves),torch.cat(self.preds)
        xs, ys = torch.cat(self.xs), torch.cat(self.ys)
        self.data.append((xs,ys,leaves,preds))
        if self.wandb: 
            fig, axs = subplots()
            self.leaf_hist(-1, ax=axs[0], show=False)
            handles, labels = axs[0].get_legend_handles_labels()
            fig.legend(handles, labels)
            wandb.log({"Leaf distribution": fig}, step = self._wandb_step)
    
    def after_fit(self):
        leaves = torch.cat(self.tree_leaves)
        idx = torch.randperm(leaves.size(0))[:self.sample_size]
        xs, ys, preds = torch.cat(self.xs), torch.cat(self.ys), torch.cat(self.preds)
        xs, ys, preds, leaves = xs[idx], ys[idx], preds[idx], leaves[idx]
        if self.wandb:
            cols = ['leaf','image','pred','target']
            data = [[l,wandb.Image(i.view(28,28)),p,y] for l,i,p,y in zip(leaves,xs,preds,ys)]
            wandb.log({"samples": wandb.Table(data=data, columns=cols)})
    
    def leaf_hist(self, epoch_idx, ax=None, show=True):
        leaves, lbls = self.data[epoch_idx][2], self.data[epoch_idx][1]
        if not ax: ax = subplots()[1][0]
        bottom, bins = torch.zeros(self.module.n_leaves), L(range(self.module.n_leaves)).map(str)
        for d in range(10):
            hist = torch.bincount(leaves[lbls==d], minlength=self.module.n_leaves)
            ax.bar(bins, hist, label=str(d),bottom=bottom)
            bottom += hist
        if show: 
             ax.legend(); plt.show()


# %% ../nbs/02_performance.ipynb 18
class GetGradCB(Callback):
    '''Get grads from module attributes'''
    def __init__(self, attrs:list, module=None): self.module, self.attrs = module, attrs
    def before_fit(self): 
        self.module = ifnone(self.module, self.learn.model)
        self.grads = {k:[] for k in self.attrs}

    def after_fit(self): self.grads = {k:torch.stack(self.grads[k]) for k in self.attrs}
    
    def after_backward(self):
        for attr in self.attrs:
            grad = getattr(getattr(self.module, attr, None), 'grad', None)
            if grad is None: continue
            self.grads[attr].append(torch.clone(grad.detach()))

    def show(self, attr, normalize=lambda x: x.abs().mean(-1), ax=None, figsize=(8,6)):
        if not ax: ax = subplots(figsize=figsize)[1][0]
        grads = normalize(self.grads[attr])
        for i in range(grads.shape[-1]): ax.plot(grads[:,i],label=i)
        ax.legend()
