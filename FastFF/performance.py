# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_performance.ipynb.

# %% auto 0
__all__ = ['FLATTEN_TFM', 'get_mnist_dls', 'runs_sweep', 'ProbsDistrCB', 'GetGradCB', 'smooth_avg']

# %% ../nbs/02_performance.ipynb 5
import wandb
from fastai.vision.all import *
from fastai.callback.wandb import *
import torchvision.transforms as T
from torchvision.datasets import MNIST

# %% ../nbs/02_performance.ipynb 7
FLATTEN_TFM = T.Lambda(lambda x: x.view(-1,28*28).squeeze())
def get_mnist_dls(bs=256, tfms=FLATTEN_TFM, num_workers=None):
    mean, std = 0.130652368068695068, 0.307504087686538696
    tfms = tfms if isinstance(tfms,list) else [tfms]
    tfm = T.Compose([T.ToTensor(),T.Normalize((mean,), (std,)), *tfms])
    train = MNIST('../data', train=True, download=True, transform=tfm)
    test = MNIST('../data', train=False, download=True, transform=tfm)
    return DataLoaders(TfmdDL(train, bs, True, num_workers),TfmdDL(test, bs, False, num_workers))

# %% ../nbs/02_performance.ipynb 10
def runs_sweep(sweep_cfg:dict, project=None, count=5, sweep_name=None):
    '''returns a decorator that runs a function in a sweep, first argument will be sweep_cfg initialized by wandb'''
    sweep_cfg['name'] = sweep_cfg.get('name', sweep_name)
    def _f(fn, *args, **kwargs):
        def run():
            wandb.init()
            print(wandb.config)
            fn(wandb.config, *args,**kwargs)
        sweep_id = wandb.sweep(sweep_cfg, project=project)
        wandb.agent(sweep_id, run, count=count)
    return _f

# %% ../nbs/02_performance.ipynb 16
class ProbsDistrCB(Callback):
    '''Gets probability distribution of module on train set and logs to wandb if enabled'''
    def __init__(self, probs_attr='probs', wandb=False, module=None, sample_size=30):
        store_attr()
 
    def before_fit(self): 
        self.module = ifnone(self.module, self.learn.model)
        if self.wandb: self._wandb_step = wandb.run.step - 1
        self.all_blocks, self.all_ys = [], []

    def before_epoch(self):  
        self.blocks, self.ys = [], []
        if self.epoch == self.n_epoch-1: self.xs, self.preds = [], []
    
    def after_batch(self): 
        if not self.training: return
        if self.wandb:  self._wandb_step += 1
        
        self.blocks.append(getattr(self.module, self.probs_attr).argmax(1).squeeze())
        self.ys.append(self.yb[0])
        if self.epoch == self.n_epoch-1:
            if not hasattr(self, 'n_blocks'): self.n_blocks = getattr(self.module, self.probs_attr).shape[-1]
            self.xs.append(self.xb[0])
            self.preds.append(self.pred.argmax(1))

    def after_epoch(self):
        self.all_blocks.append(torch.cat(self.blocks))
        self.all_ys.append(torch.cat(self.ys))
        if self.wandb: 
            fig, axs = subplots()
            self.show(-1, ax=axs[0], show=False)
            handles, labels = axs[0].get_legend_handles_labels()
            fig.legend(handles, labels)
            wandb.log({"Blocks data distribution": fig}, step = self._wandb_step)
        if self.epoch == self.n_epoch-1: 
            self.xs, self.preds = torch.cat(self.xs), torch.cat(self.preds)

    def after_fit(self):
        if not self.wandb: return
        idx = torch.randperm(len(self.xs))[:self.sample_size]
        bs, xs, ps, ys = self.all_blocks[-1][idx], self.xs[idx], self.preds[idx], self.all_ys[-1][idx]
        cols = ['block','image','pred','target']
        data = [[b, wandb.Image(x.view(28,28)), p, y] for b,x,p,y in zip(bs,xs,ps,ys)]
        wandb.log({"samples": wandb.Table(data=data, columns=cols)})
    
    def show(self, epoch_idx, ax=None, figsize=(8,6), show=True):
        blocks, ys = self.all_blocks[epoch_idx], self.all_ys[epoch_idx]
        ax = subplots(figsize=figsize)[1][0] if ax is None else ax
        bottom, bins = torch.zeros(self.n_blocks), L(range(self.n_blocks)).map(str)
        for d in range(10):
            hist = torch.bincount(blocks[ys==d], minlength=self.n_blocks)
            ax.bar(bins, hist, label=str(d),bottom=bottom)
            bottom += hist
        if show: 
             ax.legend(); plt.show()


# %% ../nbs/02_performance.ipynb 18
class GetGradCB(Callback):
    '''Get grads from modules'''
    def __init__(self, modules: list): self.modules = modules
    def before_fit(self): 
        self.grads = {m:[] for m in self.modules}

    def after_fit(self): self.grads = {k:torch.stack(v) for k, v in self.grads.items()}
    
    def after_backward(self):
        for mod in self.modules:
            grad = getattr(mod, 'weight',mod)
            grad = getattr(grad, 'grad', None)
            if grad is None: continue
            self.grads[mod].append(torch.clone(grad.detach()))

    def show(self, mod, normalize=lambda x: x.abs().mean(tuple(range(1, x.ndim))), ax=None, figsize=(8,6), beta=0.98, **kwargs):
        ax = subplots(figsize=figsize)[1][0] if ax is None else ax
        grads = smooth_avg(normalize(self.grads[mod]))
        if grads.ndim==1: grads = grads[:,None]
        for i in range(grads.shape[-1]): ax.plot(grads[:,i], label=ifnone(kwargs.pop('label',None), i), **kwargs)
        ax.legend()
        

#|export
def smooth_avg(tensor, dim=0, beta=0.95):
    tensor = tensor.swapaxes(0, dim)
    res=torch.zeros_like(tensor)
    for i in range(tensor.shape[0]):
        val = torch.lerp(tensor[i], res[max(0,i-1)], beta)
        res[i] = val
    fr = 1-beta**torch.arange(1,res.shape[0]+1)
    return (res.swapaxes(0,-1)/fr).swapaxes(0, -1).swapaxes(0, dim)

# %% ../nbs/02_performance.ipynb 34
@patch
def show_all_epochs(cb: ProbsDistrCB, n_epoch, ncols=None):
        n_epoch = 10
        ncols = ifnone(ncols, int(n_epoch**0.5))
        fig, axes = subplots(ncols=ncols, nrows=math.ceil(n_epoch/ncols), figsize=(12,5), layout="constrained")
        for i in range(n_epoch):
            ax =  axes[i//ncols][i%ncols] if math.ceil(n_epoch/ncols)>1 else axes[i%ncols]
            ax.set_yticklabels([])
            cb.show(i, ax=ax, show=False)
        handles, labels = ax.get_legend_handles_labels()
        fig.legend(handles, labels)
