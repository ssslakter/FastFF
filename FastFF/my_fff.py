# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/04_fff.ipynb.

# %% auto 0
__all__ = ['FFF']

# %% ../nbs/04_fff.ipynb 2
import torch
import torch.nn as nn, torch.nn.functional as F
from fastcore.all import *

# %% ../nbs/04_fff.ipynb 3
class FFF(nn.Module):
    def __init__(self, in_dim, out_dim, depth, act=nn.ReLU, hidden_dim = None):
        super().__init__()
        store_attr()
        self.n_leaves = 2**depth
        self.nodes = nn.Linear(in_dim, self.n_leaves-1)
        self.leaves = nn.Linear(in_dim, ifnone(hidden_dim, out_dim)*self.n_leaves)
        if hidden_dim: self.leaves_out = nn.ModuleList(nn.Linear(hidden_dim, out_dim) for _ in range(self.n_leaves))
        self.act = act() if act else noop
    
    def forward(self, x):
        bs = x.shape[0]
        probs = F.sigmoid(self.nodes(x))  # (bs, n_leaves-1)
        leaf_distr = torch.ones([bs,self.n_leaves]) # (bs, n_leaves)
        for d in range(self.depth):
            layer_probs = probs[:, 2**d-1 : 2**(d+1)-1] # (bs, 2**d)
            mask = torch.stack((1-layer_probs, layer_probs), dim=2).view(bs, -1) # (bs, 2**(d+1) )
            leaf_distr = leaf_distr.view(bs, 2**(d+1), -1) * mask[..., None] # (bs, 2**(d+1), n_leaves//2**(d+1) )
        
        logits = self.act(self.leaves(x)).view(bs,self.n_leaves, -1) # (bs, n_leaves, out_dim)
        if self.hidden_dim: logits =  self.act(torch.stack([self.leaves_out[i](logits[:,i,:]) for i in range(self.n_leaves)], dim=1))
        return torch.bmm(logits.view(bs, -1, self.n_leaves), leaf_distr.view(bs, -1,1)).view(bs, -1)
