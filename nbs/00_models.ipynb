{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp models.general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "from math import sqrt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from torch.nn.modules import ReLU\n",
    "\n",
    "\n",
    "class ExpertsBase(nn.Module):\n",
    "    def __init__(self, n_exp, in_dim, out_dim, h_dim, act=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.n_exp = n_exp\n",
    "\n",
    "        def uniform(shape, scale):\n",
    "            return nn.Parameter(torch.empty(shape).uniform_(-scale, scale))\n",
    "        self.w1 = uniform((self.n_exp, h_dim, in_dim), scale=1 / sqrt(in_dim))\n",
    "        self.w2 = uniform((self.n_exp, out_dim, h_dim), scale=1 / sqrt(h_dim))\n",
    "        self.act = act\n",
    "\n",
    "\n",
    "class TopkExperts(ExpertsBase):\n",
    "    def forward(self, x: torch.Tensor, probs: torch.Tensor = None, indices: torch.Tensor = None):\n",
    "        if indices is None: probs, indices = torch.topk(probs, self.n_exp)\n",
    "        elif len(indices.shape) == 1: indices = indices[:, None]\n",
    "        x = torch.einsum('bx,bkyx -> bky', x, self.w1[indices])\n",
    "        x = torch.einsum('bkx,bkyx -> bky', self.act(x), self.w2[indices])\n",
    "        if probs is None: return x.squeeze(1)\n",
    "        return torch.einsum('bky,bk -> by', x, probs)\n",
    "\n",
    "\n",
    "class SoftMergingExperts(ExpertsBase):\n",
    "    def forward(self, x: torch.Tensor, probs: torch.Tensor = None, indices: torch.Tensor = None):\n",
    "        \"\"\"At least one of `probs` or `indices` should be not None.\"\"\"\n",
    "        bs = x.shape[0]\n",
    "        if self.training:\n",
    "            # construct a merged expert in the first layer\n",
    "            w1 = torch.einsum('bkxy,bk->bxy', self.w1.repeat(bs, 1, 1, 1), probs)\n",
    "            x = x[:, None].bmm(w1.mT).squeeze(1)\n",
    "\n",
    "            x = self.act(x)\n",
    "\n",
    "            # similar for the next layer\n",
    "            w2 = torch.einsum('bkxy,bk->bxy', self.w2.repeat(bs, 1, 1, 1), probs)\n",
    "            x = x[:, None].bmm(w2.mT).squeeze(1)\n",
    "        else:\n",
    "            w1 = self.w1[indices]\n",
    "            x = x[:, None].bmm(w1.mT).squeeze(1)\n",
    "\n",
    "            x = self.act(x)\n",
    "\n",
    "            w2 = self.w2[indices]\n",
    "            x = x[:, None].bmm(w2.mT).squeeze(1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class GateMoE(nn.Module):\n",
    "    def __init__(self, in_dim, n_exp, top_k=1):\n",
    "        super().__init__()\n",
    "        self.n_exp, self.top_k = n_exp, top_k\n",
    "        self.gate = nn.Linear(in_dim, n_exp, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        probs, indices = torch.topk(self.gate(x), self.top_k, dim=-1)\n",
    "        probs = F.softmax(probs, dim=-1)\n",
    "        return probs, indices\n",
    "\n",
    "\n",
    "class GateMoESoft(nn.Module):\n",
    "    def __init__(self, in_dim, n_exp):\n",
    "        super().__init__()\n",
    "        self.n_exp = n_exp\n",
    "        self.gate = nn.Linear(in_dim, n_exp, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            return F.softmax(self.gate(x), dim=-1), None\n",
    "        return None, torch.argmax(self.gate(x), dim=-1)\n",
    "\n",
    "\n",
    "class GateFFF(nn.Module):\n",
    "    def __init__(self, in_dim, depth, tree_act=nn.Softplus()):\n",
    "        super().__init__()\n",
    "        self.tree_act, self.depth = tree_act, depth\n",
    "        self.n = 2**depth\n",
    "\n",
    "        def uniform(shape, scale):\n",
    "            return nn.Parameter(torch.empty(shape).uniform_(-scale, scale))\n",
    "        self.nodes = uniform((self.n - 1, in_dim), scale=1 / sqrt(in_dim))\n",
    "        self.t = self.init_t_()\n",
    "\n",
    "    def init_t_(self):\n",
    "        tree, res = torch.eye(self.n), []\n",
    "        for _ in range(self.depth):\n",
    "            res.append(tree)\n",
    "            tree = tree.view(self.n, -1, 2).sum(-1)\n",
    "        return nn.Parameter(torch.cat(list(reversed(res)), dim=1), False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        probs, indices = None, None\n",
    "        if self.training:\n",
    "            # get probs for each node\n",
    "            x = x.matmul(self.nodes.T)\n",
    "            probs = self.tree_act(torch.stack([x, -x], dim=2).view(bs, -1)).matmul(self.t.T)\n",
    "            probs = torch.softmax(probs, dim=-1)\n",
    "        else:\n",
    "            # select node with highest prob at each level\n",
    "            indices = torch.zeros(bs, dtype=torch.long, device=x.device)\n",
    "            for _ in range(self.depth):\n",
    "                indices = indices * 2 + 1 + (torch.einsum(\"bi,bi->b\", x, self.nodes[indices]) < 0).long()\n",
    "            # map to leaves that range from 0\n",
    "            indices = indices - self.n + 1\n",
    "        return probs, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class GeneralMoE(nn.Module):\n",
    "    def __init__(self, gate: nn.Module, experts: ExpertsBase, save_probs=False):\n",
    "        super().__init__()\n",
    "        self.save_probs = save_probs\n",
    "        self.gate, self.experts = gate, experts\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        probs, indices = self.gate(x)\n",
    "        if self.save_probs:\n",
    "            if probs is not None and probs.shape[-1]!=1: self.probs = probs.detach()\n",
    "            else: self.probs = F.one_hot(indices.detach(), self.experts.n_exp)\n",
    "        return self.experts(x, probs, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moe(in_dim, out_dim, d, act=nn.GELU()):\n",
    "    return GeneralMoE(GateMoE(in_dim, 2**d),\n",
    "               TopkExperts(2**d, in_dim, out_dim, d, act))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GeneralMoE(GateFFF(256, 3), TopkExperts(2**3, 256, 256, 10), save_probs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(32, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1024, -0.0772,  0.1110,  ...,  0.1453,  0.3045, -0.1895],\n",
       "        [ 0.1126, -0.1010, -0.1291,  ...,  0.1670, -0.0634, -0.0998],\n",
       "        [-0.1895, -0.1292,  0.1696,  ..., -0.2021, -0.2963,  0.3774],\n",
       "        ...,\n",
       "        [ 0.3549, -0.3767, -0.2948,  ...,  0.2112,  0.0658, -0.2372],\n",
       "        [-0.1207, -0.3709,  0.2090,  ..., -0.0537, -0.0338,  0.1172],\n",
       "        [-0.2081,  0.0433,  0.0161,  ..., -0.1541,  0.0749,  0.0009]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
