{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp models.general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "from math import sqrt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from torch.nn.modules import ReLU\n",
    "\n",
    "\n",
    "class ExpertsBase(nn.Module):\n",
    "    def __init__(self, n_exp, in_dim, out_dim, h_dim, act=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.n_exp = n_exp\n",
    "\n",
    "        def uniform(shape, scale):\n",
    "            return nn.Parameter(torch.empty(shape).uniform_(-scale, scale))\n",
    "        self.w1 = uniform((self.n_exp, h_dim, in_dim), scale=1 / sqrt(in_dim))\n",
    "        self.w2 = uniform((self.n_exp, out_dim, h_dim), scale=1 / sqrt(h_dim))\n",
    "        self.act = act\n",
    "\n",
    "\n",
    "class TopkExperts(ExpertsBase):\n",
    "    def forward(self, x: torch.Tensor, probs: torch.Tensor = None, indices: torch.Tensor = None):\n",
    "        x = torch.einsum('bx,bkyx -> bky', x, self.w1[indices])\n",
    "        x = torch.einsum('bkx,bkyx -> bky', self.act(x), self.w2[indices])\n",
    "        return torch.einsum('bky,bk -> by', x, probs)\n",
    "\n",
    "\n",
    "class SoftMergingExperts(ExpertsBase):\n",
    "    def forward(self, x: torch.Tensor, probs: torch.Tensor = None, indices: torch.Tensor = None):\n",
    "        \"\"\"At least one of `probs` or `indices` should be not None.\"\"\"\n",
    "        bs = x.shape[0]\n",
    "        if self.training:\n",
    "            # construct a merged expert in the first layer\n",
    "            w1 = torch.einsum('bkxy,bk->bxy', self.w1.repeat(bs, 1, 1, 1), probs)\n",
    "            x = x[:, None].bmm(w1.mT).squeeze(1)\n",
    "\n",
    "            x = self.act(x)\n",
    "\n",
    "            # similar for the next layer\n",
    "            w2 = torch.einsum('bkxy,bk->bxy', self.w2.repeat(bs, 1, 1, 1), probs)\n",
    "            x = x[:, None].bmm(w2.mT).squeeze(1)\n",
    "        else:\n",
    "            w1 = self.w1[indices]\n",
    "            x = x[:, None].bmm(w1.mT).squeeze(1)\n",
    "\n",
    "            x = self.act(x)\n",
    "\n",
    "            w2 = self.w2[indices]\n",
    "            x = x[:, None].bmm(w2.mT).squeeze(1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class GateMoE(nn.Module):\n",
    "    def __init__(self, in_dim, n_exp, top_k=1):\n",
    "        super().__init__()\n",
    "        self.n_exp, self.top_k = n_exp, top_k\n",
    "        self.gate = nn.Linear(in_dim, n_exp, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        probs, indices = torch.topk(self.gate(x), self.top_k, dim=-1)\n",
    "        probs = F.softmax(probs, dim=-1)\n",
    "        return probs, indices\n",
    "\n",
    "\n",
    "class GateMoESoft(nn.Module):\n",
    "    def __init__(self, in_dim, n_exp):\n",
    "        super().__init__()\n",
    "        self.n_exp = n_exp\n",
    "        self.gate = nn.Linear(in_dim, n_exp, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            return F.softmax(self.gate(x), dim=-1), None\n",
    "        return None, torch.argmax(self.gate(x), dim=-1)\n",
    "\n",
    "\n",
    "class GateFFF(nn.Module):\n",
    "    def __init__(self, in_dim, depth, tree_act=nn.Softplus()):\n",
    "        super().__init__()\n",
    "        self.tree_act, self.depth = tree_act, depth\n",
    "        self.n = 2**depth\n",
    "\n",
    "        def uniform(shape, scale):\n",
    "            return nn.Parameter(torch.empty(shape).uniform_(-scale, scale))\n",
    "        self.nodes = uniform((self.n - 1, in_dim), scale=1 / sqrt(in_dim))\n",
    "        self.t = self.init_t_()\n",
    "\n",
    "    def init_t_(self):\n",
    "        tree, res = torch.eye(self.n), []\n",
    "        for _ in range(self.depth):\n",
    "            res.append(tree)\n",
    "            tree = tree.view(self.n, -1, 2).sum(-1)\n",
    "        return nn.Parameter(torch.cat(list(reversed(res)), dim=1), False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        probs, indices = None, None\n",
    "        if self.training:\n",
    "            # get probs for each node\n",
    "            x = x.matmul(self.nodes.T)\n",
    "            probs = self.tree_act(torch.stack([x, -x], dim=2).view(bs, -1)).matmul(self.t.T)\n",
    "            probs = torch.softmax(probs, dim=-1)\n",
    "        else:\n",
    "            # select node with highest prob at each level\n",
    "            indices = torch.zeros(bs, dtype=torch.long, device=x.device)\n",
    "            for _ in range(self.depth):\n",
    "                indices = indices * 2 + 1 + (torch.einsum(\"bi,bi->b\", x, self.nodes[indices]) < 0).long()\n",
    "            # map to leaves that range from 0\n",
    "            indices = indices - self.n + 1\n",
    "        return probs, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class GeneralMoE(nn.Module):\n",
    "    def __init__(self, gate: nn.Module, experts: ExpertsBase, save_probs=False):\n",
    "        super().__init__()\n",
    "        self.save_probs = save_probs\n",
    "        self.gate, self.experts = gate, experts\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        probs, indices = self.gate(x)\n",
    "        if self.save_probs:\n",
    "            if probs is not None: self.probs = probs.detach()\n",
    "            else: self.probs = F.one_hot(indices.detach(), self.experts.n_exp)\n",
    "        return self.experts(x, probs, indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
