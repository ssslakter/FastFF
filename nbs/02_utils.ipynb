{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import wandb\n",
    "from fastai.vision.all import *\n",
    "from fastai.callback.wandb import *\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import MNIST, CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic dataset methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "FLATTEN_TFM = T.Lambda(lambda x: x.view(-1,28*28).squeeze())\n",
    "def get_mnist_dls(bs=256, num_workers=None, tfms=FLATTEN_TFM):\n",
    "    mean, std = 0.130652368068695068, 0.307504087686538696\n",
    "    tfms = tfms if isinstance(tfms,list) else [tfms]\n",
    "    tfm = T.Compose([T.ToTensor(),T.Normalize((mean,), (std,)), *tfms])\n",
    "    train = MNIST('../data', train=True, download=True, transform=tfm)\n",
    "    test = MNIST('../data', train=False, download=True, transform=tfm)\n",
    "    return DataLoaders(TfmdDL(train, bs, True, num_workers),TfmdDL(test, bs, False, num_workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "FLATTEN_CIFAR_TFM = [T.Lambda(lambda x: x.view(-1,32*32*3).squeeze())]\n",
    "def get_cifar_dls(bs=64, num_workers=None, tfms=FLATTEN_CIFAR_TFM):\n",
    "    mean, std = (0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)\n",
    "    tfms = T.Compose([T.ToTensor(),T.Normalize(mean, std), *tfms])\n",
    "    train = CIFAR10('../data', train=True, download=True, transform=tfms)\n",
    "    test = CIFAR10('../data', train=False, download=True, transform=tfms)\n",
    "    return DataLoaders(TfmdDL(train, bs, True, num_workers),TfmdDL(test, bs, False, num_workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def runs_sweep(sweep_cfg:dict, project=None, count=5, sweep_name=None, sweep_id=None):\n",
    "    '''returns a decorator that runs a function in a sweep, first argument will be sweep_cfg initialized by wandb'''\n",
    "    sweep_cfg['name'] = sweep_cfg.get('name', sweep_name)\n",
    "    def _f(fn, *args, **kwargs):\n",
    "        def run():\n",
    "            with wandb.init() as run:\n",
    "                fn(wandb.config, run, *args,**kwargs)\n",
    "        nonlocal sweep_id\n",
    "        if sweep_id is None: sweep_id = wandb.sweep(sweep_cfg, project=project)\n",
    "        wandb.agent(sweep_id, run, count=count)\n",
    "    return _f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure callabacks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TinyWandbCallback(WandbCallback): pass\n",
    "\n",
    "@patch\n",
    "def gather_args(self:Learner):\n",
    "    \"Gather config parameters accessible to the learner\"\n",
    "    args = {}\n",
    "    # input dimensions\n",
    "    try:\n",
    "        n_inp = self.dls.train.n_inp\n",
    "        xb = self.dls.valid.one_batch()[:n_inp]\n",
    "        args.update({f'input dim {i+1}':d for n in range(n_inp) for i,d in enumerate(list(detuplify(xb[n]).shape))})\n",
    "    except: print(f'Could not gather input dimensions')\n",
    "    # other useful information\n",
    "    with ignore_exceptions():\n",
    "        args['batch per epoch'] = len(self.dls.train)\n",
    "        args['model parameters'] = total_params(self.model)[0]\n",
    "        args['device'] = self.dls.device.type\n",
    "        args['frozen'] = bool(self.opt.frozen_idx)\n",
    "        args['frozen idx'] = self.opt.frozen_idx\n",
    "        args['dataset.tfms'] = f'{self.dls.dataset.tfms}'\n",
    "        args['dls.after_item'] = f'{self.dls.after_item}'\n",
    "        args['dls.before_batch'] = f'{self.dls.before_batch}'\n",
    "        args['dls.after_batch'] = f'{self.dls.after_batch}'\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callback to extract expert distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ProbsDistrCB(Callback):\n",
    "    '''Gets probability distribution of module on train set and logs to wandb if enabled'''\n",
    "    def __init__(self, probs_attr='probs', wandb=False, module=None, sample_size=30):\n",
    "        store_attr()\n",
    " \n",
    "    def before_fit(self): \n",
    "        self.module = ifnone(self.module, self.learn.model)\n",
    "        if self.wandb: self._wandb_step = wandb.run.step - 1\n",
    "        self.all_blocks, self.all_ys = [], []\n",
    "\n",
    "    def before_epoch(self):  \n",
    "        self.blocks, self.ys = [], []\n",
    "        if self.epoch == self.n_epoch-1: self.xs, self.preds = [], []\n",
    "    \n",
    "    def after_batch(self): \n",
    "        if not self.training: return\n",
    "        if self.wandb:  self._wandb_step += 1\n",
    "        if not hasattr(self, 'n_blocks'): self.n_blocks = getattr(self.module, self.probs_attr).squeeze().shape[-1]\n",
    "\n",
    "        self.blocks.append(getattr(self.module, self.probs_attr).argmax(1).squeeze())\n",
    "        self.ys.append(self.yb[0])\n",
    "        if self.epoch == self.n_epoch-1:\n",
    "            self.xs.append(self.xb[0])\n",
    "            self.preds.append(self.pred.argmax(1))\n",
    "\n",
    "    def after_epoch(self):\n",
    "        self.all_blocks.append(torch.cat(self.blocks))\n",
    "        self.all_ys.append(torch.cat(self.ys))\n",
    "        if self.wandb: \n",
    "            fig, axs = subplots()\n",
    "            self.show(-1, ax=axs[0], show=False)\n",
    "            handles, labels = axs[0].get_legend_handles_labels()\n",
    "            fig.legend(handles, labels)\n",
    "            wandb.log({\"Blocks data distribution\": fig}, step = self._wandb_step)\n",
    "        if self.epoch == self.n_epoch-1: \n",
    "            self.xs, self.preds = torch.cat(self.xs), torch.cat(self.preds)\n",
    "\n",
    "    def after_fit(self):\n",
    "        if not self.wandb: return\n",
    "        idx = torch.randperm(len(self.xs))[:self.sample_size]\n",
    "        bs, xs, ps, ys = self.all_blocks[-1][idx], self.xs[idx], self.preds[idx], self.all_ys[-1][idx]\n",
    "        cols = ['block','image','pred','target']\n",
    "        data = [[b, wandb.Image(x.view(28,28)), p, y] for b,x,p,y in zip(bs,xs,ps,ys)]\n",
    "        wandb.log({\"samples\": wandb.Table(data=data, columns=cols)})\n",
    "    \n",
    "    def show(self, epoch_idx, ax=None, figsize=(8,6), show=True):\n",
    "        blocks, ys = self.all_blocks[epoch_idx], self.all_ys[epoch_idx]\n",
    "        ax = subplots(figsize=figsize)[1][0] if ax is None else ax\n",
    "        bottom, bins = torch.zeros(self.n_blocks), L(range(self.n_blocks)).map(str)\n",
    "        for d in range(10):\n",
    "            hist = to_cpu(torch.bincount(blocks[ys==d], minlength=self.n_blocks))\n",
    "            ax.bar(bins, hist, label=str(d),bottom=bottom)\n",
    "            bottom += hist\n",
    "        if show: \n",
    "             ax.legend(); plt.show()\n",
    "\n",
    "\n",
    "    def show_all_epochs(self, n_epoch, ncols=None):\n",
    "        n_epoch = 10\n",
    "        ncols = ifnone(ncols, int(n_epoch**0.5))\n",
    "        fig, axes = subplots(ncols=ncols, nrows=math.ceil(n_epoch/ncols), figsize=(12,5), layout=\"constrained\")\n",
    "        for i in range(n_epoch):\n",
    "            ax =  axes[i//ncols][i%ncols] if math.ceil(n_epoch/ncols)>1 else axes[i%ncols]\n",
    "            ax.set_yticklabels([])\n",
    "            self.show(i, ax=ax, show=False)\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        fig.legend(handles, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class GetGradCB(Callback):\n",
    "    '''Get grads from modules'''\n",
    "    def __init__(self, modules: list): self.modules = modules\n",
    "    def before_fit(self): \n",
    "        self.grads = {m:[] for m in self.modules}\n",
    "\n",
    "    def after_fit(self): self.grads = {k:torch.stack(v) for k, v in self.grads.items()}\n",
    "    \n",
    "    def after_backward(self):\n",
    "        for mod in self.modules:\n",
    "            grad = getattr(mod, 'weight',mod)\n",
    "            grad = getattr(grad, 'grad', None)\n",
    "            if grad is None: continue\n",
    "            self.grads[mod].append(torch.clone(grad.detach()))\n",
    "\n",
    "    def show(self, mod, normalize=lambda x: x.abs().mean(tuple(range(1, x.ndim))), ax=None, figsize=(8,6), beta=0.98, **kwargs):\n",
    "        ax = subplots(figsize=figsize)[1][0] if ax is None else ax\n",
    "        grads = smooth_avg(normalize(self.grads[mod])).cpu().numpy()\n",
    "        if grads.ndim==1: grads = grads[:,None]\n",
    "        for i in range(grads.shape[-1]): ax.plot(grads[:,i], label=ifnone(kwargs.pop('label',None), i), **kwargs)\n",
    "        ax.legend()\n",
    "        \n",
    "\n",
    "#|export\n",
    "def smooth_avg(tensor, dim=0, beta=0.95):\n",
    "    tensor = tensor.swapaxes(0, dim)\n",
    "    res=torch.zeros_like(tensor, device=tensor.device)\n",
    "    for i in range(tensor.shape[0]):\n",
    "        val = torch.lerp(tensor[i], res[max(0,i-1)], beta)\n",
    "        res[i] = val\n",
    "    fr = 1-beta**torch.arange(1,res.shape[0]+1, device=res.device)\n",
    "    return (res.swapaxes(0,-1)/fr).swapaxes(0, -1).swapaxes(0, dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
