{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastFF\n",
    "> Some experiments with FFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This repository contains experiments comparing Mixture-of-Experts (MoE) and Fast Feed-Forward (FFF) models introduced in FFF and UltraFastBert papers ([author's repository](https://github.com/pbelcak/fastfeedforward)).\n",
    "\n",
    "The `experiments` folder contains (almost) self-contained Jupyter notebooks with benchmarks and experiments with the architecture.\n",
    "\n",
    "The `FastFF` folder contains several implementations of the FFF model, including the reference one, with additional tools to get data from models and train them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "Use pip or other package manager to install the package from this repository\n",
    "```sh\n",
    "pip install git+https://github.com/ssslakter/FastFF\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "The main results are:\n",
    "\n",
    "- [SMEAR](http://arxiv.org/abs/2306.03745) gives slight improvements in the FFF model as well as MoE, although the hierarchical structure makes it harder to train. [Jupyter notebook](experiments/22-05-2024-Soft-merging.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data distribution between experts shifts to the single peak when increasing the number of neurons in the experts. [Jupyter notebook](nbs/01_mnist.ipynb)\n",
    "\n",
    "<figure>\n",
    "  <img width=\"500\" src=\"images/distr.png\" id=\"jupyter\"/>\n",
    "  <figcaption>Distribution of data between $2^4$ experts for classification task with 10 classes</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FFF can be formulated as a MoE with a __sparse binary matrix__ of transitions and additional activation function (_Softplus_ in the reference formulation). Additional experiments show that linear activation function performs better. [Jupyter notebook](experiments/29-05-2024-FFF-activation.ipynb)\n",
    "\n",
    "<figure>\n",
    "    <img width=\"600\" src=\"images/act.png\" id=\"jupyter\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With matrix formulation, the utilization of parallelism is higher than in the reference implementation, therefore for shallow layers there is a speedup. For deep layers the sequential branch selection becomes faster, when dense matrices require lots of space.[Jupyter notebook](experiments/23-04-2024-FFF-probs-benchmark.ipynb)\n",
    "\n",
    "<figure>\n",
    "    <img width=\"600\" src=\"images/bench.png\" id=\"jupyter\"/>\n",
    "</figure>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
