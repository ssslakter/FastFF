{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn.functional as F, math, wandb, os\n",
    "from torch import nn, tensor\n",
    "from fastcore.all import *\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "from fastprogress import progress_bar, master_bar\n",
    "from torch.utils import benchmark\n",
    "\n",
    "os.environ[\"WANDB_SILENT\"]='true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model, x, cuda=False):\n",
    "    if cuda: model,x = model.cuda(), x.cuda()\n",
    "    return benchmark.Timer(\n",
    "            stmt='model(x)',\n",
    "            globals=locals(),\n",
    "            label=model._get_name(),\n",
    "            description='time',\n",
    "        ).blocked_autorange(min_run_time=0.5)\n",
    "\n",
    "def benchmark_with_params(func, x, params: list, cuda=False):\n",
    "    '''func must accept single parameter from params and return model for `benchmark_model`'''\n",
    "    results= L((benchmark_model(func(p), x, cuda)) for p in progress_bar(params, parent=globals().get('mb',None)))\n",
    "    # take mean and convert to ms\n",
    "    return results.map(lambda x: x.mean*1e3)\n",
    "\n",
    "def keys_to_str(dict, keys): \n",
    "    return ' '.join(f'{k}={v}' for k,v in dict.items() if k in keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MoE with sequential expert computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experts(nn.ModuleList):\n",
    "    def forward(self, x, routing_ws, selected_exps):\n",
    "        mask = F.one_hot(selected_exps, num_classes=len(self)).permute(2, 1, 0)\n",
    "        for i in range(len(self)):\n",
    "            idx, top_x = torch.where(mask[i])\n",
    "            if top_x.shape[0] == 0: continue\n",
    "            # in torch it is faster to index using lists than torch tensors\n",
    "            top_x_list = top_x.tolist()\n",
    "            res = self[i](x[top_x_list]) * routing_ws[top_x_list, idx.tolist(), None]\n",
    "            if 'out' not in locals(): out = torch.zeros((x.shape[0],*res.shape[1:]), device=x.device)\n",
    "            out.index_add_(0, top_x, res)\n",
    "        return out\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, h_dim, n_exp=4, topk=2, act=nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.topk=topk\n",
    "        self.gate = nn.Linear(in_dim, n_exp, bias=False)\n",
    "        self.experts = Experts(\n",
    "            nn.Sequential(\n",
    "            nn.Linear(in_dim, h_dim, bias=False), act(),\n",
    "            nn.Linear(h_dim, out_dim, bias=False)) for _ in range(n_exp))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        logits = self.gate(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        probs, selected_exps = torch.topk(probs, self.topk, dim=-1)\n",
    "        probs /= probs.sum(dim=-1, keepdim=True)\n",
    "        return self.experts(x, probs, selected_exps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MoE with parrallel expert computation by materializing matrix of size b\\*k\\*x\\*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoeEinops(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, h_dim, n_exp=4, topk=2, act=nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.topk = topk\n",
    "        def init_uniform(shape, scale):\n",
    "            return nn.Parameter(torch.empty(shape).uniform_(-scale, scale))\n",
    "        self.gate = init_uniform((n_exp, in_dim), 1/sqrt(in_dim))\n",
    "        self.w1 = init_uniform((n_exp, h_dim, in_dim), 1/sqrt(in_dim))\n",
    "        self.w2 = init_uniform((n_exp, out_dim, h_dim), 1/sqrt(h_dim))\n",
    "        self.act = act()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        probs, indices = torch.matmul(x, self.gate.T).topk(self.topk)\n",
    "        probs = torch.softmax(probs, dim=-1)\n",
    "        x = torch.einsum('bx,bkyx -> bky', x, self.w1[indices])\n",
    "        x = torch.einsum('bkx,bkyx -> bky', self.act(x), self.w2[indices])\n",
    "        return torch.einsum('bky,bk -> by', x, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vram():\n",
    "    free = torch.cuda.mem_get_info()[0] / 1024 ** 3\n",
    "    total = torch.cuda.mem_get_info()[1] / 1024 ** 3\n",
    "    total_cubes = 24\n",
    "    free_cubes = int(total_cubes * free / total)\n",
    "    return f'VRAM: {total - free:.2f}/{total:.2f}GB\\t VRAM:[' + (\n",
    "            total_cubes - free_cubes) * '▮' + free_cubes * '▯' + ']'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topk=1, similar speed to inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=32\n",
    "h_dims = [1,2,4,8,16,32,64,128,256,512,1024]\n",
    "n_exps = [1,2,3,4,8,16,32,64,128]\n",
    "params = dict(\n",
    "    in_dim=28*28,\n",
    "    out_dim=10,\n",
    "    n_exp=128,\n",
    "    topk=128)\n",
    "x = torch.randn(bs, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.profiler.profile() as p:\n",
    "    _, indices = torch.matmul(x, self.gate.T).topk(params['topk'])\n",
    "    \n",
    "p.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def profile_model(model, x, out_dir = './tracing'):\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(exist_ok=True, parents=True)\n",
    "    with torch.profiler.profile(record_shapes=True, profile_memory=True, with_stack=True) as p:\n",
    "        for _ in range(5): \n",
    "            with torch.no_grad(): model(x)\n",
    "    prefix = f\"{int(time.time())}\"\n",
    "    p.export_chrome_trace(str(out_dir/f'{model._get_name()}_{prefix}_trace.json.gz'))\n",
    "    p.export_memory_timeline(str(out_dir/f'{model._get_name()}_{prefix}_memory.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(32, 28*28).cuda()\n",
    "\n",
    "moe1 = MoeEinops(h_dim=1024, **params).cuda()\n",
    "moe2 = MoE(h_dim=1024, **params).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM: 1.87/6.00GB\t VRAM:[▮▮▮▮▮▮▮▮▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯]\n",
      "VRAM: 1.87/6.00GB\t VRAM:[▮▮▮▮▮▮▮▮▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯]\n",
      "VRAM: 6.00/6.00GB\t VRAM:[▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0687,  0.0018,  0.0147,  0.0180,  0.0042, -0.0061,  0.0039, -0.0313,\n",
       "         -0.0343,  0.0185],\n",
       "        [ 0.0147,  0.0136,  0.0317, -0.0256,  0.0145, -0.0191, -0.0766, -0.0188,\n",
       "         -0.0352,  0.0211],\n",
       "        [ 0.0219,  0.0048,  0.0178,  0.0030,  0.0388, -0.0516, -0.0235, -0.0017,\n",
       "          0.0270,  0.0173],\n",
       "        [-0.0002, -0.0012, -0.0017,  0.0201, -0.0248, -0.0227, -0.0117, -0.0080,\n",
       "         -0.0079, -0.0459],\n",
       "        [-0.0037,  0.0371,  0.0374, -0.0031, -0.0044, -0.0186,  0.0012,  0.0122,\n",
       "          0.0044, -0.0092],\n",
       "        [-0.0230,  0.0049, -0.0164, -0.0175,  0.0309, -0.0090, -0.0230, -0.0600,\n",
       "          0.0093,  0.0154],\n",
       "        [-0.0348,  0.0199,  0.0182, -0.0442,  0.0100, -0.0137, -0.0299, -0.0215,\n",
       "         -0.0003, -0.0371],\n",
       "        [ 0.0299,  0.0113,  0.0158, -0.0520,  0.0088,  0.0127,  0.0525, -0.0184,\n",
       "         -0.0172,  0.0123],\n",
       "        [-0.0037, -0.0178,  0.0240,  0.0033, -0.0014, -0.0332,  0.0384, -0.0072,\n",
       "         -0.0680,  0.0155],\n",
       "        [ 0.0119, -0.0217,  0.0308, -0.0154,  0.0054, -0.0167, -0.0330,  0.0013,\n",
       "         -0.0158, -0.0475],\n",
       "        [ 0.0224, -0.0269,  0.0234,  0.0023,  0.0235, -0.0210, -0.0168, -0.0015,\n",
       "         -0.0060,  0.0185],\n",
       "        [-0.0185, -0.0292,  0.0252, -0.0549,  0.0236, -0.0099,  0.0247,  0.0042,\n",
       "          0.0002, -0.0098],\n",
       "        [-0.0125,  0.0014,  0.0326, -0.0223,  0.0519, -0.0228, -0.0066, -0.0118,\n",
       "          0.0352, -0.0041],\n",
       "        [ 0.0496,  0.0331,  0.0222,  0.0012,  0.0306, -0.0225,  0.0242, -0.0197,\n",
       "          0.0083, -0.0162],\n",
       "        [-0.0273,  0.0249,  0.0211, -0.0389,  0.0202, -0.0448, -0.0155, -0.0107,\n",
       "         -0.0282,  0.0028],\n",
       "        [ 0.0283, -0.0323,  0.0012, -0.0346, -0.0195, -0.0291, -0.0241, -0.0054,\n",
       "          0.0076,  0.0298],\n",
       "        [ 0.0192,  0.0013,  0.0218, -0.0088,  0.0003, -0.0027,  0.0299, -0.0064,\n",
       "         -0.0214, -0.0208],\n",
       "        [ 0.0174,  0.0112, -0.0047,  0.0014,  0.0605, -0.0145, -0.0232, -0.0170,\n",
       "         -0.0036,  0.0288],\n",
       "        [-0.0107,  0.0212,  0.0022, -0.0111,  0.0121, -0.0013, -0.0233, -0.0151,\n",
       "         -0.0041, -0.0366],\n",
       "        [ 0.0163,  0.0401, -0.0359, -0.0279,  0.0218, -0.0016, -0.0156, -0.0026,\n",
       "         -0.0118,  0.0298],\n",
       "        [-0.0048,  0.0283,  0.0021, -0.0129,  0.0055,  0.0041,  0.0190,  0.0027,\n",
       "         -0.0257, -0.0052],\n",
       "        [-0.0092,  0.0447,  0.0182, -0.0093, -0.0008,  0.0136,  0.0038,  0.0157,\n",
       "          0.0183,  0.0187],\n",
       "        [-0.0028,  0.0353,  0.0561, -0.0024, -0.0249, -0.0466, -0.0510, -0.0217,\n",
       "         -0.0102,  0.0241],\n",
       "        [-0.0099,  0.0275, -0.0326, -0.0051,  0.0014, -0.0574, -0.0197,  0.0082,\n",
       "          0.0039, -0.0051],\n",
       "        [ 0.0346,  0.0020,  0.0080, -0.0467, -0.0250, -0.0073,  0.0057,  0.0158,\n",
       "         -0.0040, -0.0008],\n",
       "        [ 0.0105, -0.0046,  0.0234, -0.0497,  0.0077, -0.0648, -0.0116, -0.0180,\n",
       "          0.0272, -0.0215],\n",
       "        [ 0.0094, -0.0047,  0.0047, -0.0468,  0.0195, -0.0083, -0.0117, -0.0079,\n",
       "         -0.0065, -0.0472],\n",
       "        [-0.0283,  0.0016,  0.0414, -0.0561,  0.0390, -0.0006, -0.0038, -0.0107,\n",
       "         -0.0360,  0.0008],\n",
       "        [ 0.0048,  0.0040, -0.0059, -0.0443,  0.0002, -0.0083, -0.0254, -0.0311,\n",
       "          0.0072, -0.0498],\n",
       "        [ 0.0162,  0.0602, -0.0027,  0.0220, -0.0030,  0.0027,  0.0233, -0.0092,\n",
       "         -0.0049,  0.0113],\n",
       "        [-0.0151, -0.0184,  0.0335, -0.0609,  0.0098,  0.0006,  0.0073, -0.0125,\n",
       "          0.0078,  0.0010],\n",
       "        [-0.0288,  0.0040,  0.0261, -0.0066,  0.0171, -0.0259, -0.0628, -0.0113,\n",
       "         -0.0123,  0.0332]], device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moe1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1571.7456319998746"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_model(moe1, x, True).mean*1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204.85662466671783"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_model(moe2, x, True).mean*1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_model(moe2, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_model(moe1, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_benchmark(topk=1, cuda=False):\n",
    "    global mb\n",
    "    mb = master_bar(n_exps, total=len(n_exps))\n",
    "    for n_exp in mb:\n",
    "        params['n_exp'] = n_exp\n",
    "        params['topk'] = min(topk, n_exp)\n",
    "        models = {\"MoE einops\": lambda x: MoeEinops(h_dim=x, **params),\n",
    "                \"MoE sequential\":  lambda x: MoE(h_dim=x, **params)}\n",
    "        results = {k:[] for k in models}\n",
    "        for n,m in models.items():\n",
    "            dev = 'cuda' if cuda else 'cpu'\n",
    "            wandb.init(project='FFF', config = params | {'model':n}, name=f\"{dev} {keys_to_str(params, ['n_exp','topk'])} {n}\")\n",
    "            results[n] =  benchmark_with_params(m, x, h_dims, cuda)\n",
    "            for h,t in zip(h_dims, results[n]): wandb.log({\"time\": t, \"h_dim\": h})\n",
    "            wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU, then CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_benchmark(cuda=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topk=n, similar to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_benchmark(topk=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_benchmark(topk=1024, cuda=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
