{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn.functional as F, math, wandb, os\n",
    "from torch import nn, tensor\n",
    "from fastcore.all import *\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "from fastprogress import progress_bar, master_bar\n",
    "from torch.utils import benchmark\n",
    "\n",
    "os.environ[\"WANDB_SILENT\"]='true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model, x, cuda=False):\n",
    "    if cuda: model,x = model.cuda(), x.cuda()\n",
    "    return benchmark.Timer(\n",
    "            stmt='model(x)',\n",
    "            globals=locals(),\n",
    "            label=model._get_name(),\n",
    "            description='time',\n",
    "        ).blocked_autorange(min_run_time=0.2)\n",
    "\n",
    "def benchmark_with_params(func, x, params: list, cuda=False):\n",
    "    '''func must accept single parameter from params and return model for `benchmark_model`'''\n",
    "    results= L((benchmark_model(func(p), x, cuda)) for p in progress_bar(params, parent=globals().get('mb',None)))\n",
    "    # take mean and convert to ms\n",
    "    return results.map(lambda x: x.mean*1e3)\n",
    "\n",
    "def keys_to_str(dict, keys): \n",
    "    return ' '.join(f'{k}={v}' for k,v in dict.items() if k in keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MoE with sequential expert computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experts(nn.ModuleList):\n",
    "    def forward(self, x, routing_ws, selected_exps):\n",
    "        mask = F.one_hot(selected_exps, num_classes=len(self)).permute(2, 1, 0)\n",
    "        for i in range(len(self)):\n",
    "            idx, top_x = torch.where(mask[i])\n",
    "            if top_x.shape[0] == 0: continue\n",
    "            # in torch it is faster to index using lists than torch tensors\n",
    "            top_x_list = top_x.tolist()\n",
    "            res = self[i](x[top_x_list]) * routing_ws[top_x_list, idx.tolist(), None]\n",
    "            if 'out' not in locals(): out = torch.zeros((x.shape[0],*res.shape[1:]), device=x.device)\n",
    "            out.index_add_(0, top_x, res)\n",
    "        return out\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, h_dim, n_exp=4, topk=2, act=nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.topk=topk\n",
    "        self.gate = nn.Linear(in_dim, n_exp, bias=False)\n",
    "        self.experts = Experts(\n",
    "            nn.Sequential(\n",
    "            nn.Linear(in_dim, h_dim, bias=False), act(),\n",
    "            nn.Linear(h_dim, out_dim, bias=False)) for _ in range(n_exp))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        logits = self.gate(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        probs, selected_exps = torch.topk(probs, self.topk, dim=-1)\n",
    "        probs /= probs.sum(dim=-1, keepdim=True)\n",
    "        return self.experts(x, probs, selected_exps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MoE with parrallel expert computation by materializing matrix of size b\\*k\\*x\\*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoeEinops(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, h_dim, n_exp=4, topk=2, act=nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.topk = topk\n",
    "        def init_uniform(shape, scale):\n",
    "            return nn.Parameter(torch.empty(shape).uniform_(-scale, scale))\n",
    "        self.gate = init_uniform((n_exp, in_dim), 1/sqrt(in_dim))\n",
    "        self.w1 = init_uniform((n_exp, h_dim, in_dim), 1/sqrt(in_dim))\n",
    "        self.w2 = init_uniform((n_exp, out_dim, h_dim), 1/sqrt(h_dim))\n",
    "        self.act = act()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        probs, indices = torch.matmul(x, self.gate.T).topk(self.topk)\n",
    "        probs = torch.softmax(probs, dim=-1)\n",
    "        x = torch.einsum('bx,bkyx -> bky', x, self.w1[indices])\n",
    "        x = torch.einsum('bkx,bkyx -> bky', self.act(x), self.w2[indices])\n",
    "        return torch.einsum('bky,bk -> by', x, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topk=1, similar speed to inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=32\n",
    "h_dims = [1,2,4,8,16,32,64,128,256,512,1024]\n",
    "n_exps = [1,2,3,4,8,16,32,64,128]\n",
    "params = dict(\n",
    "    in_dim=28*28,\n",
    "    out_dim=10,\n",
    "    n_exp=1,\n",
    "    topk=1)\n",
    "x = torch.randn(bs, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_benchmark(topk=1, cuda=False):\n",
    "    global mb\n",
    "    mb = master_bar(n_exps, total=len(n_exps))\n",
    "    for n_exp in mb:\n",
    "        params['n_exp'] = n_exp\n",
    "        params['topk'] = min(topk, n_exp)\n",
    "        models = {\"MoE einops\": lambda x: MoeEinops(h_dim=x, **params),\n",
    "                \"MoE sequential\":  lambda x: MoE(h_dim=x, **params)}\n",
    "        results = {k:[] for k in models}\n",
    "        for n,m in models.items():\n",
    "            dev = 'cuda' if cuda else 'cpu'\n",
    "            wandb.init(project='FFF', config = params | {'model':n}, name=f\"{dev} {keys_to_str(params, ['n_exp','topk'])} {n}\")\n",
    "            results[n] =  benchmark_with_params(m, x, h_dims, cuda)\n",
    "            for h,t in zip(h_dims, results[n]): wandb.log({\"time\": t, \"h_dim\": h})\n",
    "            wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU, then CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_benchmark(cuda=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topk=n, similar to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_benchmark(topk=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_benchmark(topk=1024, cuda=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
