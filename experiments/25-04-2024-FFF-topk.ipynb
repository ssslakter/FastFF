{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.functional as F\n",
    "from torch import nn, tensor\n",
    "from fastcore.all import *\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFF(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, h_dim, depth, tree_act=nn.LogSigmoid(), topk=None, save_probs=True):\n",
    "        super().__init__()\n",
    "        store_attr('save_probs,tree_act,depth')\n",
    "        self.n = 2**depth\n",
    "        self.topk = topk or self.n\n",
    "        def uniform(shape, scale): \n",
    "            return nn.Parameter(torch.empty(shape).uniform_(-scale,scale))\n",
    "        self.nodes = uniform((self.n-1, in_dim), scale=1/sqrt(in_dim))\n",
    "        self.w1 = uniform((self.n, h_dim, in_dim), scale=1/sqrt(in_dim))\n",
    "        self.w2 = uniform((self.n, out_dim, h_dim), scale=1/sqrt(h_dim))\n",
    "        self.act = nn.ReLU()\n",
    "        self.t = self.init_t_()\n",
    "        self.s = self.init_s_()\n",
    "\n",
    "    def init_t_(self):\n",
    "        tree, res = torch.eye(self.n), []\n",
    "        for _ in range(self.depth): \n",
    "            res.append(tree)\n",
    "            tree = tree.view(self.n, -1, 2).sum(-1)\n",
    "        return nn.Parameter(torch.cat(list(reversed(res)),dim=1), False)\n",
    "\n",
    "    def init_s_(self):\n",
    "        s = torch.eye(self.n-1)\n",
    "        return nn.Parameter(torch.stack([s,-s], dim=2).view(self.n-1,2*(self.n-1)), False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        if self.training:\n",
    "            z = x.matmul(self.nodes.T).matmul(self.s)\n",
    "            z = self.tree_act(z).matmul(self.t.T)\n",
    "            if self.save_probs: self.probs = torch.softmax(z,-1)\n",
    "            probs, indices = z.topk(self.topk)\n",
    "            probs = torch.softmax(probs, dim=-1)\n",
    "        else:\n",
    "            indices = torch.zeros(bs, dtype=torch.long, device=x.device)\n",
    "            for _ in range(self.depth):\n",
    "                indices = indices*2 + 1 + (torch.einsum(\"b i, b i -> b\", x, self.nodes[indices])<0).long()\n",
    "            indices = indices[:,None] - self.n+1\n",
    "            probs = torch.ones(bs,1)\n",
    "        x = torch.einsum('bx, bkyx -> bky', x, self.w1[indices])\n",
    "        x = torch.einsum('bkx, bkyx -> bky', self.act(x), self.w2[indices])\n",
    "        return torch.einsum('bky, bk -> by', x, probs) if probs.shape[1]>1 else x[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experts(nn.ModuleList):\n",
    "    def forward(self, x, routing_ws, selected_exps):\n",
    "        mask = F.one_hot(selected_exps, num_classes=len(self)).permute(2, 1, 0)\n",
    "        for i in range(len(self)):\n",
    "            idx, top_x = torch.where(mask[i])\n",
    "            if top_x.shape[0] == 0: continue\n",
    "            # in torch it is faster to index using lists than torch tensors\n",
    "            top_x_list = top_x.tolist()\n",
    "            res = self[i](x[top_x_list]) * routing_ws[top_x_list, idx.tolist(), None]\n",
    "            if 'out' not in locals(): out = torch.zeros((x.shape[0],*res.shape[1:]), device=x.device)\n",
    "            out.index_add_(0, top_x, res)\n",
    "        return out\n",
    "    \n",
    "def binary(x, bits):\n",
    "    'converts integer vector into binary with number of `bits`'\n",
    "    mask = 2**torch.arange(bits, device=x.device, dtype=x.dtype)\n",
    "    return x.unsqueeze(-1).bitwise_and(mask).ne(0).byte()\n",
    "\n",
    "class FFFSeq(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, h_dim, depth, tree_act=nn.LogSigmoid(), topk=None, save_probs=True):\n",
    "        super().__init__()\n",
    "        store_attr('save_probs,tree_act,depth')\n",
    "        self.n = 2**depth\n",
    "        self.topk = topk or self.n\n",
    "        def uniform(shape, scale): \n",
    "            return nn.Parameter(torch.empty(shape).uniform_(-scale,scale))\n",
    "        self.nodes = uniform((self.n-1, in_dim), scale=1/sqrt(in_dim))\n",
    "        self.leaves = Experts(\n",
    "            nn.Sequential(\n",
    "            nn.Linear(in_dim, h_dim, bias=False), nn.ReLU(),\n",
    "            nn.Linear(h_dim, out_dim, bias=False)) for _ in range(self.n))\n",
    "        self.t = self.init_t_()\n",
    "        self.s = self.init_s_()\n",
    "\n",
    "    def init_t_(self):\n",
    "        tree, res = torch.eye(self.n), []\n",
    "        for _ in range(self.depth): \n",
    "            res.append(tree)\n",
    "            tree = tree.view(self.n, -1, 2).sum(-1)\n",
    "        return nn.Parameter(torch.cat(list(reversed(res)),dim=1), False)\n",
    "\n",
    "    def init_s_(self):\n",
    "        s = torch.eye(self.n-1)\n",
    "        return nn.Parameter(torch.stack([s,-s], dim=2).view(self.n-1,2*(self.n-1)), False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        if self.training:\n",
    "            z = x.matmul(self.nodes.T).matmul(self.s)\n",
    "            z = self.tree_act(z).matmul(self.t.T)\n",
    "            if self.save_probs: self.probs = torch.softmax(z,-1)\n",
    "            probs, indices = z.topk(self.topk)\n",
    "            probs = torch.softmax(probs, dim=-1)\n",
    "        else:\n",
    "            indices = torch.zeros(bs, dtype=torch.long, device=x.device)\n",
    "            for _ in range(self.depth):\n",
    "                indices = indices*2 + 1 + (torch.einsum(\"b i, b i -> b\", x, self.nodes[indices])<0).long()\n",
    "            indices = indices[:,None] - self.n+1\n",
    "            probs = torch.ones(bs,1)\n",
    "        return self.leaves(x, probs, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callback.wandb import *\n",
    "from FastFF.utils import *\n",
    "from fastai.vision.all import *\n",
    "\n",
    "bs = 512\n",
    "params = dict(\n",
    "    in_dim=28*28,\n",
    "    out_dim=10,\n",
    "    h_dim=16,\n",
    "    depth=3,\n",
    "    topk=2)\n",
    "\n",
    "fff = FFF(**params)\n",
    "cbs = [ShowGraphCallback(), ProbsDistrCB(), GetGradCB([fff.nodes, fff.w1])]\n",
    "dls = get_mnist_dls(bs)\n",
    "Learner(dls, fff, loss_func=F.cross_entropy, metrics=accuracy, cbs=cbs).fit_one_cycle(5, lr_max=7e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 512\n",
    "params = dict(\n",
    "    in_dim=28*28,\n",
    "    out_dim=10,\n",
    "    h_dim=16,\n",
    "    depth=3,\n",
    "    topk=2)\n",
    "\n",
    "fff = FFFSeq(**params)\n",
    "cbs = [ShowGraphCallback(), ProbsDistrCB()]\n",
    "dls = get_mnist_dls(bs)\n",
    "Learner(dls, fff, loss_func=F.cross_entropy, metrics=accuracy, cbs=cbs).fit_one_cycle(5, lr_max=7e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
